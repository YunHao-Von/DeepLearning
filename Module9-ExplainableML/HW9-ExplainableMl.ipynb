{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.segmentation import slic\n",
    "from lime import lime_image\n",
    "from pdb import set_trace\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'ckptpath': './model.pth',\n",
    "    'dataset_dir': './Data/food/'\n",
    "}\n",
    "args = argparse.Namespace(**args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Classifier(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Classifier, self).__init__()\n",
    "\n",
    "    def building_block(indim, outdim):\n",
    "      return [\n",
    "          nn.Conv2d(indim, outdim, 3, 1, 1),\n",
    "          nn.BatchNorm2d(outdim),\n",
    "          nn.ReLU(),\n",
    "      ]\n",
    "\n",
    "    def stack_blocks(indim, outdim, block_num):\n",
    "      layers = building_block(indim, outdim)\n",
    "      for i in range(block_num - 1):\n",
    "        layers += building_block(outdim, outdim)\n",
    "      layers.append(nn.MaxPool2d(2, 2, 0))\n",
    "      return layers\n",
    "\n",
    "    cnn_list = []\n",
    "    cnn_list += stack_blocks(3, 128, 3)\n",
    "    cnn_list += stack_blocks(128, 128, 3)\n",
    "    cnn_list += stack_blocks(128, 256, 3)\n",
    "    cnn_list += stack_blocks(256, 512, 1)\n",
    "    cnn_list += stack_blocks(512, 512, 1)\n",
    "    self.cnn = nn.Sequential(* cnn_list)\n",
    "\n",
    "    dnn_list = [\n",
    "        nn.Linear(512 * 4 * 4, 1024),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(1024, 11),\n",
    "    ]\n",
    "    self.fc = nn.Sequential(* dnn_list)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.cnn(x)\n",
    "    out = out.reshape(out.size()[0], -1)\n",
    "    return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = Classifier().cuda()\n",
    "checkpoint = torch.load(args.ckptpath)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# It should display: <All keys matched successfully> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It might take some time, if it is too long, try to reload it.\n",
    "# Dataset definition\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, paths, labels, mode):\n",
    "        # mode: 'train' or 'eval'\n",
    "\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        trainTransform = transforms.Compose([\n",
    "            transforms.Resize(size=(128, 128)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        evalTransform = transforms.Compose([\n",
    "            transforms.Resize(size=(128, 128)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.transform = trainTransform if mode == 'train' else evalTransform\n",
    "\n",
    "    # pytorch dataset class\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = Image.open(self.paths[index])\n",
    "        X = self.transform(X)\n",
    "        Y = self.labels[index]\n",
    "        return X, Y\n",
    "\n",
    "    # help to get images for visualizing\n",
    "    def getbatch(self, indices):\n",
    "        images = []\n",
    "        labels = []\n",
    "        for index in indices:\n",
    "          image, label = self.__getitem__(index)\n",
    "          images.append(image)\n",
    "          labels.append(label)\n",
    "        return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "# help to get data path and label\n",
    "\n",
    "\n",
    "def get_paths_labels(path):\n",
    "    def my_key(name):\n",
    "      return int(name.replace(\".jpg\", \"\"))+1000000*int(name.split(\"_\")[0])\n",
    "    imgnames = os.listdir(path)\n",
    "    imgnames.sort(key=my_key)\n",
    "    imgpaths = []\n",
    "    labels = []\n",
    "    for name in imgnames:\n",
    "        imgpaths.append(os.path.join(path, name))\n",
    "        labels.append(int(name.split('_')[0]))\n",
    "    return imgpaths, labels\n",
    "\n",
    "\n",
    "train_paths, train_labels = get_paths_labels(args.dataset_dir)\n",
    "\n",
    "train_set = FoodDataset(train_paths, train_labels, mode='eval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_indices = [i for i in range(10)]\n",
    "images, labels = train_set.getbatch(img_indices)\n",
    "fig, axs = plt.subplots(1, len(img_indices), figsize=(15, 8))\n",
    "for i, img in enumerate(images):\n",
    "  axs[i].imshow(img.cpu().permute(1, 2, 0))\n",
    "# print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    # input: numpy array, (batches, height, width, channels)\n",
    "\n",
    "    model.eval()\n",
    "    input = torch.FloatTensor(input).permute(0, 3, 1, 2)\n",
    "    # pytorch tensor, (batches, channels, height, width)\n",
    "\n",
    "    output = model(input.cuda())\n",
    "    return output.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def segmentation(input):\n",
    "    # split the image into 200 pieces with the help of segmentaion from skimage\n",
    "    return slic(input, n_segments=200, compactness=1, sigma=1)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, len(img_indices), figsize=(15, 8))\n",
    "# fix the random seed to make it reproducible\n",
    "np.random.seed(16)\n",
    "for idx, (image, label) in enumerate(zip(images.permute(0, 2, 3, 1).numpy(), labels)):\n",
    "    x = image.astype(np.double)\n",
    "    # numpy array for lime\n",
    "\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    explaination = explainer.explain_instance(\n",
    "        image=x, classifier_fn=predict, segmentation_fn=segmentation)\n",
    "\n",
    "    # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=explain_instance#lime.lime_image.LimeImageExplainer.explain_instance\n",
    "\n",
    "    lime_img, mask = explaination.get_image_and_mask(\n",
    "        label=label.item(),\n",
    "        positive_only=False,\n",
    "        hide_rest=False,\n",
    "        num_features=11,\n",
    "        min_weight=0.05\n",
    "    )\n",
    "    # turn the result from explainer to the image\n",
    "    # doc: https://lime-ml.readthedocs.io/en/latest/lime.html?highlight=get_image_and_mask#lime.lime_image.ImageExplanation.get_image_and_mask\n",
    "\n",
    "    axs[idx].imshow(lime_img)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "  return (image - image.min()) / (image.max() - image.min())\n",
    "  # return torch.log(image)/torch.log(image.max())\n",
    "\n",
    "\n",
    "def compute_saliency_maps(x, y, model):\n",
    "  model.eval()\n",
    "  x = x.cuda()\n",
    "\n",
    "  # we want the gradient of the input x\n",
    "  x.requires_grad_()\n",
    "\n",
    "  y_pred = model(x)\n",
    "  loss_func = torch.nn.CrossEntropyLoss()\n",
    "  loss = loss_func(y_pred, y.cuda())\n",
    "  loss.backward()\n",
    "\n",
    "  # saliencies = x.grad.abs().detach().cpu()\n",
    "  saliencies, _ = torch.max(x.grad.data.abs().detach().cpu(), dim=1)\n",
    "\n",
    "  # We need to normalize each image, because their gradients might vary in scale, but we only care about the relation in each image\n",
    "  saliencies = torch.stack([normalize(item) for item in saliencies])\n",
    "  return saliencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = train_set.getbatch(img_indices)\n",
    "saliencies = compute_saliency_maps(images, labels, model)\n",
    "\n",
    "# visualize\n",
    "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
    "for row, target in enumerate([images, saliencies]):\n",
    "  for column, img in enumerate(target):\n",
    "    if row == 0:\n",
    "      axs[row][column].imshow(img.permute(1, 2, 0).numpy())\n",
    "      # What is permute?\n",
    "      # In pytorch, the meaning of each dimension of image tensor is (channels, height, width)\n",
    "      # In matplotlib, the meaning of each dimension of image tensor is (height, width, channels)\n",
    "      # permute is a tool for permuting dimensions of tensors\n",
    "      # For example, img.permute(1, 2, 0) means that,\n",
    "      # - 0 dimension is the 1 dimension of the original tensor, which is height\n",
    "      # - 1 dimension is the 2 dimension of the original tensor, which is width\n",
    "      # - 2 dimension is the 0 dimension of the original tensor, which is channels\n",
    "    else:\n",
    "      axs[row][column].imshow(img.numpy(), cmap=plt.cm.hot)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth grad\n",
    "\n",
    "def normalize(image):\n",
    "  return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "\n",
    "def smooth_grad(x, y, model, epoch, param_sigma_multiplier):\n",
    "  model.eval()\n",
    "  #x = x.cuda().unsqueeze(0)\n",
    "\n",
    "  mean = 0\n",
    "  sigma = param_sigma_multiplier / (torch.max(x) - torch.min(x)).item()\n",
    "  smooth = np.zeros(x.cuda().unsqueeze(0).size())\n",
    "  for i in range(epoch):\n",
    "    # call Variable to generate random noise\n",
    "    noise = Variable(x.data.new(x.size()).normal_(mean, sigma**2))\n",
    "    x_mod = (x+noise).unsqueeze(0).cuda()\n",
    "    x_mod.requires_grad_()\n",
    "\n",
    "    y_pred = model(x_mod)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_func(y_pred, y.cuda().unsqueeze(0))\n",
    "    loss.backward()\n",
    "\n",
    "    # like the method in saliency map\n",
    "    smooth += x_mod.grad.abs().detach().cpu().data.numpy()\n",
    "  smooth = normalize(smooth / epoch)  # don't forget to normalize\n",
    "  # smooth = smooth / epoch\n",
    "  return smooth\n",
    "\n",
    "\n",
    "# images, labels = train_set.getbatch(img_indices)\n",
    "smooth = []\n",
    "for i, l in zip(images, labels):\n",
    "  smooth.append(smooth_grad(i, l, model, 500, 0.4))\n",
    "smooth = np.stack(smooth)\n",
    "print(smooth.shape)\n",
    "\n",
    "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
    "for row, target in enumerate([images, smooth]):\n",
    "  for column, img in enumerate(target):\n",
    "    axs[row][column].imshow(np.transpose(img.reshape(3, 128, 128), (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images, labels = train_set.getbatch(img_indices)\n",
    "saliencies = compute_saliency_maps(images, labels, model)\n",
    "\n",
    "# visualize\n",
    "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
    "for row, target in enumerate([images, saliencies]):\n",
    "  for column, img in enumerate(target):\n",
    "    if row == 0:\n",
    "      axs[row][column].imshow(img.permute(1, 2, 0).numpy())\n",
    "      # What is permute?\n",
    "      # In pytorch, the meaning of each dimension of image tensor is (channels, height, width)\n",
    "      # In matplotlib, the meaning of each dimension of image tensor is (height, width, channels)\n",
    "      # permute is a tool for permuting dimensions of tensors\n",
    "      # For example, img.permute(1, 2, 0) means that,\n",
    "      # - 0 dimension is the 1 dimension of the original tensor, which is height\n",
    "      # - 1 dimension is the 2 dimension of the original tensor, which is width\n",
    "      # - 2 dimension is the 0 dimension of the original tensor, which is channels\n",
    "    else:\n",
    "      axs[row][column].imshow(img.numpy(), cmap=plt.cm.hot)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth grad\n",
    "\n",
    "def normalize(image):\n",
    "  return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "\n",
    "def smooth_grad(x, y, model, epoch, param_sigma_multiplier):\n",
    "  model.eval()\n",
    "  #x = x.cuda().unsqueeze(0)\n",
    "\n",
    "  mean = 0\n",
    "  sigma = param_sigma_multiplier / (torch.max(x) - torch.min(x)).item()\n",
    "  smooth = np.zeros(x.cuda().unsqueeze(0).size())\n",
    "  for i in range(epoch):\n",
    "    # call Variable to generate random noise\n",
    "    noise = Variable(x.data.new(x.size()).normal_(mean, sigma**2))\n",
    "    x_mod = (x+noise).unsqueeze(0).cuda()\n",
    "    x_mod.requires_grad_()\n",
    "\n",
    "    y_pred = model(x_mod)\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_func(y_pred, y.cuda().unsqueeze(0))\n",
    "    loss.backward()\n",
    "\n",
    "    # like the method in saliency map\n",
    "    smooth += x_mod.grad.abs().detach().cpu().data.numpy()\n",
    "  smooth = normalize(smooth / epoch)  # don't forget to normalize\n",
    "  # smooth = smooth / epoch\n",
    "  return smooth\n",
    "\n",
    "\n",
    "# images, labels = train_set.getbatch(img_indices)\n",
    "smooth = []\n",
    "for i, l in zip(images, labels):\n",
    "  smooth.append(smooth_grad(i, l, model, 500, 0.4))\n",
    "smooth = np.stack(smooth)\n",
    "print(smooth.shape)\n",
    "\n",
    "fig, axs = plt.subplots(2, len(img_indices), figsize=(15, 8))\n",
    "for row, target in enumerate([images, smooth]):\n",
    "  for column, img in enumerate(target):\n",
    "    axs[row][column].imshow(np.transpose(img.reshape(3, 128, 128), (1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "  return (image - image.min()) / (image.max() - image.min())\n",
    "\n",
    "\n",
    "layer_activations = None\n",
    "\n",
    "\n",
    "def filter_explanation(x, model, cnnid, filterid, iteration=100, lr=1):\n",
    "  # x: input image\n",
    "  # cnnid, filterid: cnn layer id, which filter\n",
    "  model.eval()\n",
    "\n",
    "  def hook(model, input, output):\n",
    "    global layer_activations\n",
    "    layer_activations = output\n",
    "\n",
    "  hook_handle = model.cnn[cnnid].register_forward_hook(hook)\n",
    "  # When the model forward through the layer[cnnid], need to call the hook function first\n",
    "  # The hook function save the output of the layer[cnnid]\n",
    "  # After forwarding, we'll have the loss and the layer activation\n",
    "\n",
    "  # Filter activation: x passing the filter will generate the activation map\n",
    "  model(x.cuda())  # forward\n",
    "\n",
    "  # Based on the filterid given by the function argument, pick up the specific filter's activation map\n",
    "  # We just need to plot it, so we can detach from graph and save as cpu tensor\n",
    "  filter_activations = layer_activations[:, filterid, :, :].detach().cpu()\n",
    "\n",
    "  # Filter visualization: find the image that can activate the filter the most\n",
    "  x = x.cuda()\n",
    "  x.requires_grad_()\n",
    "  # input image gradient\n",
    "  optimizer = Adam([x], lr=lr)\n",
    "  # Use optimizer to modify the input image to amplify filter activation\n",
    "  for iter in range(iteration):\n",
    "    optimizer.zero_grad()\n",
    "    model(x)\n",
    "\n",
    "    objective = -layer_activations[:, filterid, :, :].sum()\n",
    "    # We want to maximize the filter activation's summation\n",
    "    # So we add a negative sign\n",
    "\n",
    "    objective.backward()\n",
    "    # Calculate the partial differential value of filter activation to input image\n",
    "    optimizer.step()\n",
    "    # Modify input image to maximize filter activation\n",
    "  filter_visualizations = x.detach().cpu().squeeze()\n",
    "\n",
    "  # Don't forget to remove the hook\n",
    "  hook_handle.remove()\n",
    "  # The hook will exist after the model register it, so you have to remove it after used\n",
    "  # Just register a new hook if you want to use it\n",
    "\n",
    "  return filter_activations, filter_visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train_set.getbatch(img_indices)\n",
    "filter_activations, filter_visualizations = filter_explanation(\n",
    "    images, model, cnnid=6, filterid=0, iteration=100, lr=0.1)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3, len(img_indices), figsize=(15, 8))\n",
    "for i, img in enumerate(images):\n",
    "  axs[0][i].imshow(img.permute(1, 2, 0))\n",
    "# Plot filter activations\n",
    "for i, img in enumerate(filter_activations):\n",
    "  axs[1][i].imshow(normalize(img))\n",
    "# Plot filter visualization\n",
    "for i, img in enumerate(filter_visualizations):\n",
    "  axs[2][i].imshow(normalize(img.permute(1, 2, 0)))\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# 從下面四張圖可以看到，activate 的區域對應到一些物品的邊界，尤其是顏色對比較深的邊界\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train_set.getbatch(img_indices)\n",
    "filter_activations, filter_visualizations = filter_explanation(\n",
    "    images, model, cnnid=23, filterid=0, iteration=100, lr=0.1)\n",
    "\n",
    "# Plot filter activations\n",
    "fig, axs = plt.subplots(3, len(img_indices), figsize=(15, 8))\n",
    "for i, img in enumerate(images):\n",
    "  axs[0][i].imshow(img.permute(1, 2, 0))\n",
    "for i, img in enumerate(filter_activations):\n",
    "  axs[1][i].imshow(normalize(img))\n",
    "for i, img in enumerate(filter_visualizations):\n",
    "  axs[2][i].imshow(normalize(img.permute(1, 2, 0)))\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d12a6980af1de3549060b7b451d48d445ec6b4aaeaf0b0e12a509d2182e95745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
