{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ-C2Dgetg37"
      },
      "source": [
        "# **Homework 6 - Generative Adversarial Network**\n",
        "\n",
        "This is the example code of homework 6 of the machine learning course by Prof. Hung-yi Lee.\n",
        "\n",
        "\n",
        "In this homework, you are required to build a generative adversarial  network for anime face generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTBkY5QFf3QM"
      },
      "source": [
        "## Set up the environment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7y4wyYdEABR"
      },
      "source": [
        "### Packages Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IQB485dD_eL"
      },
      "outputs": [],
      "source": [
        "# You may replace the workspace directory if you want.\n",
        "workspace_dir = '.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjfM46dtmxXj"
      },
      "source": [
        "## Random seed\n",
        "Set the random seed to a certain value for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWuecW1imz42"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def same_seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCTPz2iRQmwe"
      },
      "source": [
        "## Import Packages\n",
        "First, we need to import packages that will be used later.\n",
        "\n",
        "Like hw3, we highly rely on **torchvision**, a library of PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TC8RRsX0QhL-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from qqdm.notebook import qqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYjZ_G83_YX4"
      },
      "source": [
        "## Dataset\n",
        "1. Resize the images to (64, 64)\n",
        "1. Linearly map the values from [0, 1] to  [-1, 1].\n",
        "\n",
        "Please refer to [PyTorch official website](https://pytorch.org/vision/stable/transforms.html) for details about different transforms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ6d0_cr8R26"
      },
      "outputs": [],
      "source": [
        "class CrypkoDataset(Dataset):\n",
        "    def __init__(self, fnames, transform):\n",
        "        self.transform = transform\n",
        "        self.fnames = fnames\n",
        "        self.num_samples = len(self.fnames)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        fname = self.fnames[idx]\n",
        "        # 1. Load the image\n",
        "        img = torchvision.io.read_image(fname)\n",
        "        # 2. Resize and normalize the images using torchvision.\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "def get_dataset(root):\n",
        "    fnames = glob.glob(os.path.join(root, '*'))\n",
        "    # 1. Resize the image to (64, 64)\n",
        "    # 2. Linearly map [0, 1] to [-1, 1]\n",
        "    compose = [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((64, 64)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "    ]\n",
        "    transform = transforms.Compose(compose)\n",
        "    dataset = CrypkoDataset(fnames, transform)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGwdVhOKSjLY"
      },
      "source": [
        "### Show some images\n",
        "Note that the values are in the range of [-1, 1], we should shift them to the valid range, [0, 1], to display correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34mVNtHn7cwF",
        "outputId": "5a657b52-ce4d-4f5a-e5a8-d15d0408be5f"
      },
      "outputs": [],
      "source": [
        "dataset = get_dataset(os.path.join(workspace_dir, 'dataset'))\n",
        "\n",
        "images = [dataset[i] for i in range(16)]\n",
        "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhxUjRUuHdti",
        "outputId": "b8277a3b-fe40-485c-c004-1652dba27dd3"
      },
      "outputs": [],
      "source": [
        "images = [(dataset[i]+1)/2 for i in range(16)]\n",
        "grid_img = torchvision.utils.make_grid(images, nrow=4)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkvZ4JgCHCZD"
      },
      "source": [
        "## Model\n",
        "Here, we use DCGAN as the model structure. Feel free to modify your own model structure.\n",
        "\n",
        "Note that the `N` of the input/output shape stands for the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0I1jRd6HFmm"
      },
      "outputs": [],
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input shape: (N, in_dim)\n",
        "    Output shape: (N, 3, 64, 64)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "        super(Generator, self).__init__()\n",
        "        def dconv_bn_relu(in_dim, out_dim):\n",
        "            return nn.Sequential(\n",
        "                nn.ConvTranspose2d(in_dim, out_dim, 5, 2,\n",
        "                                   padding=2, output_padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_dim),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Linear(in_dim, dim * 8 * 4 * 4, bias=False),\n",
        "            nn.BatchNorm1d(dim * 8 * 4 * 4),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.l2_5 = nn.Sequential(\n",
        "            dconv_bn_relu(dim * 8, dim * 4),\n",
        "            dconv_bn_relu(dim * 4, dim * 2),\n",
        "            dconv_bn_relu(dim * 2, dim),\n",
        "            nn.ConvTranspose2d(dim, 3, 5, 2, padding=2, output_padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.l1(x)\n",
        "        y = y.view(y.size(0), -1, 4, 4)\n",
        "        y = self.l2_5(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Input shape: (N, 3, 64, 64)\n",
        "    Output shape: (N, )\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def conv_bn_lrelu(in_dim, out_dim):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_dim, out_dim, 5, 2, 2),\n",
        "                nn.BatchNorm2d(out_dim),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            )\n",
        "            \n",
        "        \"\"\" Medium: Remove the last sigmoid layer for WGAN. \"\"\"\n",
        "        self.ls = nn.Sequential(\n",
        "            nn.Conv2d(in_dim, dim, 5, 2, 2), \n",
        "            nn.LeakyReLU(0.2),\n",
        "            conv_bn_lrelu(dim, dim * 2),\n",
        "            conv_bn_lrelu(dim * 2, dim * 4),\n",
        "            conv_bn_lrelu(dim * 4, dim * 8),\n",
        "            nn.Conv2d(dim * 8, 1, 4),\n",
        "            nn.Sigmoid(), \n",
        "        )\n",
        "        self.apply(weights_init)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        y = self.ls(x)\n",
        "        y = y.view(-1)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxo4teqaO5RJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5sCGIUtSViC"
      },
      "source": [
        "### Initialization\n",
        "- hyperparameters\n",
        "- model\n",
        "- optimizer\n",
        "- dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EqomOouHezf"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "batch_size = 64\n",
        "z_dim = 100\n",
        "z_sample = Variable(torch.randn(100, z_dim)).cuda()\n",
        "lr = 1e-4\n",
        "\n",
        "\"\"\" Medium: WGAN, 50 epoch, n_critic=5, clip_value=0.01 \"\"\"\n",
        "n_epoch = 1000 # 50\n",
        "n_critic = 5 # 5\n",
        "clip_value = 0.01\n",
        "\n",
        "log_dir = os.path.join(workspace_dir, 'logs')\n",
        "ckpt_dir = os.path.join(workspace_dir, 'checkpoints')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "# Model\n",
        "G = Generator(in_dim=z_dim).cuda()\n",
        "D = Discriminator(3).cuda()\n",
        "G.train()\n",
        "D.train()\n",
        "\n",
        "# Loss\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\"\"\" Medium: Use RMSprop for WGAN. \"\"\"\n",
        "# Optimizer\n",
        "opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "opt_G = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "# opt_D = torch.optim.RMSprop(D.parameters(), lr=lr)\n",
        "# opt_G = torch.optim.RMSprop(G.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "# DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpJA1wzi0tii"
      },
      "source": [
        "### Training loop\n",
        "We store some pictures regularly to monitor the current \bperformance of the Generator, and regularly record checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678,
          "referenced_widgets": [
            "565d67d9e799493fa2197dda1d06c4ea",
            "53faf655a44d43abb5b1df6a5724917a",
            "1806f82990df4176a0c62425baf83311",
            "a478d938977740079eda33017c5e3c10",
            "09d49355e7414445bee8e9a4b521bfe7",
            "40710bba15b64b9ab454a2d5ad630e03",
            "0dc308b6988f413fb12fa046c39a0c4e",
            "034c8ea9a887489f8c5ba191035255f2"
          ]
        },
        "id": "dgkqPih1o5Az",
        "outputId": "29a89ab4-4e6a-4ebf-e845-0d5ee3b1a424"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "steps = 0\n",
        "for e, epoch in enumerate(range(n_epoch)):\n",
        "    progress_bar = tqdm(dataloader)\n",
        "    for data in progress_bar:\n",
        "        imgs = data\n",
        "        imgs = imgs.cuda()\n",
        "\n",
        "        bs = imgs.size(0)\n",
        "\n",
        "        # ============================================\n",
        "        #  Train D\n",
        "        # ============================================\n",
        "        z = Variable(torch.randn(bs, z_dim)).cuda()\n",
        "        r_imgs = Variable(imgs).cuda()\n",
        "        f_imgs = G(z)\n",
        "\n",
        "        \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
        "        # Label\n",
        "        r_label = torch.ones((bs)).cuda()\n",
        "        f_label = torch.zeros((bs)).cuda()\n",
        "\n",
        "        # Model forwarding\n",
        "        r_logit = D(r_imgs.detach())\n",
        "        f_logit = D(f_imgs.detach())\n",
        "        \n",
        "        # Compute the loss for the discriminator.\n",
        "        r_loss = criterion(r_logit, r_label)\n",
        "        f_loss = criterion(f_logit, f_label)\n",
        "        loss_D = (r_loss + f_loss) / 2\n",
        "\n",
        "        # WGAN Loss\n",
        "        # loss_D = -torch.mean(D(r_imgs)) + torch.mean(D(f_imgs))\n",
        "       \n",
        "\n",
        "        # Model backwarding\n",
        "        D.zero_grad()\n",
        "        loss_D.backward()\n",
        "\n",
        "        # Update the discriminator.\n",
        "        opt_D.step()\n",
        "\n",
        "        \"\"\" Medium: Clip weights of discriminator. \"\"\"\n",
        "        # for p in D.parameters():\n",
        "        #    p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "        # ============================================\n",
        "        #  Train G\n",
        "        # ============================================\n",
        "        # if steps % n_critic == 0:\n",
        "        #     # Generate some fake images.\n",
        "        #     z = Variable(torch.randn(bs, z_dim)).cuda()\n",
        "        #     f_imgs = G(z)\n",
        "\n",
        "        #     # Model forwarding\n",
        "        #     f_logit = D(f_imgs)\n",
        "            \n",
        "        #     \"\"\" Medium: Use WGAN Loss\"\"\"\n",
        "        #     # Compute the loss for the generator.\n",
        "        #     loss_G = criterion(f_logit, r_label)\n",
        "        #     # WGAN Loss\n",
        "        #     # loss_G = -torch.mean(D(f_imgs))\n",
        "\n",
        "        #     # Model backwarding\n",
        "        #     G.zero_grad()\n",
        "        #     loss_G.backward()\n",
        "\n",
        "        #     # Update the generator.\n",
        "        #     opt_G.step()\n",
        "\n",
        "        steps += 1\n",
        "        \n",
        "        # Set the info of the progress bar\n",
        "        #   Note that the value of the GAN loss is not directly related to\n",
        "        #   the quality of the generated images.\n",
        "\n",
        "    G.eval()\n",
        "    f_imgs_sample = (G(z_sample).data + 1) / 2.0\n",
        "    filename = os.path.join(log_dir, f'Epoch_{epoch+1:03d}.jpg')\n",
        "    torchvision.utils.save_image(f_imgs_sample, filename, nrow=10)\n",
        "    print(f' | Save some samples to {filename}.')\n",
        "    \n",
        "    # Show generated images in the jupyter notebook.\n",
        "    # grid_img = torchvision.utils.make_grid(f_imgs_sample.cpu(), nrow=10)\n",
        "    # plt.figure(figsize=(10,10))\n",
        "    # plt.imshow(grid_img.permute(1, 2, 0))\n",
        "    # plt.show()\n",
        "    G.train()\n",
        "\n",
        "    if (e+1) % 5 == 0 or e == 0:\n",
        "        # Save the checkpoints.\n",
        "        torch.save(G.state_dict(), os.path.join(ckpt_dir, 'G.pth'))\n",
        "        torch.save(D.state_dict(), os.path.join(ckpt_dir, 'D.pth'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2uJFmTtKBeH"
      },
      "source": [
        "## Inference\n",
        "Use the trained model to generate anime faces!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXPXcVD_HJB2"
      },
      "source": [
        "### Load model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JnQdNx2SUS2",
        "outputId": "3148dfab-79e4-480a-e740-bdd81ad53380"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "G = Generator(z_dim)\n",
        "G.load_state_dict(torch.load(os.path.join(ckpt_dir, 'G.pth')))\n",
        "G.eval()\n",
        "G.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I8PDocbHQiN"
      },
      "source": [
        "### Generate and show some images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-SYKrRea_-Q",
        "outputId": "aba269bb-3ea7-4df9-a30a-569107f522c1"
      },
      "outputs": [],
      "source": [
        "# Generate 1000 images and make a grid to save them.\n",
        "n_output = 1000\n",
        "z_sample = Variable(torch.randn(n_output, z_dim)).cuda()\n",
        "imgs_sample = (G(z_sample).data + 1) / 2.0\n",
        "log_dir = os.path.join(workspace_dir, 'logs')\n",
        "filename = os.path.join(log_dir, 'result.jpg')\n",
        "torchvision.utils.save_image(imgs_sample, filename, nrow=10)\n",
        "\n",
        "# Show 32 of the images.\n",
        "grid_img = torchvision.utils.make_grid(imgs_sample[:32].cpu(), nrow=10)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B04ATOTHc4F"
      },
      "source": [
        "### Compress the generated images using **tar**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbcmoTQpz_yf",
        "outputId": "c2ef944b-b701-47b2-94b8-80a7aab36163"
      },
      "outputs": [],
      "source": [
        "# Save the generated images.\n",
        "os.makedirs('output', exist_ok=True)\n",
        "for i in range(1000):\n",
        "    torchvision.utils.save_image(imgs_sample[i], f'output/{i+1}.jpg')\n",
        "  \n",
        "# Compress the images.\n",
        "%cd output\n",
        "!tar -zcf ../images.tgz *.jpg\n",
        "%cd .."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Y7y4wyYdEABR",
        "2NFjuZTPDxLn"
      ],
      "name": "hw6_GAN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "d12a6980af1de3549060b7b451d48d445ec6b4aaeaf0b0e12a509d2182e95745"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "034c8ea9a887489f8c5ba191035255f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d49355e7414445bee8e9a4b521bfe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dc308b6988f413fb12fa046c39a0c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1806f82990df4176a0c62425baf83311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40710bba15b64b9ab454a2d5ad630e03",
            "placeholder": "​",
            "style": "IPY_MODEL_09d49355e7414445bee8e9a4b521bfe7",
            "value": "100.0%"
          }
        },
        "40710bba15b64b9ab454a2d5ad630e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53faf655a44d43abb5b1df6a5724917a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "565d67d9e799493fa2197dda1d06c4ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1806f82990df4176a0c62425baf83311",
              "IPY_MODEL_a478d938977740079eda33017c5e3c10"
            ],
            "layout": "IPY_MODEL_53faf655a44d43abb5b1df6a5724917a"
          }
        },
        "a478d938977740079eda33017c5e3c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_034c8ea9a887489f8c5ba191035255f2",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0dc308b6988f413fb12fa046c39a0c4e",
            "value": 100
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
