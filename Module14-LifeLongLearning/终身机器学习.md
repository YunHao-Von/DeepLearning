# 14.终身学习  
## 概述
由旧数据训练好一个模型，这个模型在遇到新数据的时候，使用新数据来更新自己的参数。     
多任务同时学习的效果比依序学习多任务的效果相对较好。   
多任务学习的问题: 消耗时间和内存过大。  
终身学习的关注点:  
机器依序学习多个任务，并且要尽量在每个任务上的表现结果较好。  
衡量终身学习的效果:  
|            | 任务1    | 任务2    | ... | 任务T    |
| ---------- | -------- | -------- | --- | -------- |
| 随机初始化 | R(0,1)   | R(0,2)   |     | R(0,T)   |
| 任务1      | R(1,1)   | R(1,2)   |     | R(1,T)   |
| 任务2      | R(2,1)   | R(2,2)   |     | R(2,T)   |
| ...        |          |          |     |          |
| 任务T-1    | R(T-1,1) | R(T-1,2) |     | R(T-1,T) |
| 任务T      | R(T,1)   | R(T,2)   |     | R(T,T)   |

R(i,j):当模型在第i个任务上完成训练后，再将模型拿去测试第j个任务得到的效果。  
+ i>j:  
    当模型在i任务上训练完成后，其关于第j个任务的遗忘程度。  
+ i<j:  
    当模型在i任务上训练完成后，没看过第j个任务，直接transfer在第j个任务上。   

评估终身学习模型的三种方式:  
+ $Accuracy = \frac{1}{T}\sum_{i=1}^TR_{T,i}$  
+ $Backward Transfer = \frac{1}{T-1}\sum_{i=1}^{T-1}R_{T,i}-R_{i,i}$  
  Backward Transfer的值通常是负的。  

+ $Forward Transfer =\frac{1}{T-1}\sum_{i=2}^{T}R_{i-1,i}-R_{0,i}$  

## 终身学习的解法:  
### 1.突触可塑性(selective synaptic plasticity)  
只让某些神经元在学习新任务的时候可以被更新。   
Catastrophic Forgetting:模型依序学习表现结果极差。  

基本的想法:  
每个参数对于任务的意义是不一样的，我们希望对于已经学习到的任务意义重大的参数，其值不会改变。而只修改那些对于过去任务不重要的参数即可。  

每一个参数$\theta_i^b$都会有一个守卫$b_i$   
$b_i$的意义是表明这个参数有多重要。
同时改写损失函数:  
$L'(\theta) = L(\theta) + \lambda \sum_i b_i(\theta_i-\theta_i^b)^2$  

在新改写的损失函数中，越重要的参数其对应的b也就越大，同时迫使其两参数差异越小。不重要的其实可以直接把b赋值为0。  
在终身学习的任务中，最重要的就是如何设置$b_i$  

### 2.Additional Neural Resource Allocation  
Progressive Netork:每多加一个任务，就多开一部分任务。   
PackNet：直接生成一个大网络，每个任务只使用其一部分。  
结合就程CPG。  

### 3.Memory Reply  
前列任务的模型生成数据和后续任务一起训练。  
